\documentclass[11pt]{article}
\usepackage{a4wide,parskip}
\usepackage{hyperref}
\usepackage{titlesec}

\titleformat{\section}{\normalfont\fontsize{12}{15}\bfseries}{\thesection}{1em}{}

\begin{document}

\centerline{\Large R209 Essay:  Correctness vs. Mitigation}
\vspace{2em}
\centerline{\large Chongyang Shi (\emph{cs940})}
\vspace{1em}
\centerline{\large \today}
\vspace{1em}

\section{Summaries of research}

The paper by Klein et al. \cite{klein2009sel4} discussed the design and construction process of a formally verified general-purpose L4 microkernel -- \emph{seL4}. In order to ensure the functional-correctness of the microkernel, the authors took very thorough care when designing seL4's programming model to minimise complexities introduced into verification. A three-level refinement process was used -- a Haskell-based executable model implemented the full functionality of the microkernel, as defined by an abstract specification. The executable model was then optimised for performance by reimplementing in C. Isabelle/HOL was used in an interactive verification process to ensure the model's functional-correctness. The resulting microkernel is comparable in performance to other L4 kernels, and imposes a reasonable materialistic cost in verification. As a potential improvement to the paper, more discussions could be included on the impact of the programming model's design decisions on the programmer's ability to write concise and efficient code for the kernel.

The article by Bessey et al. \cite{bessey2010few} described notable observations and unique challenges faced by the authors in the process of commercialising a static analysis-based bug-finding tool. Their product applied an unsound traversal on the target codebase to find as many errors as possible. The authors identified some caveats in the process, including integrating with customised build systems, dealing with compilers and runtime environments not in compliance with language standards, and working with misinformed users as well as commercial demands. They also introduced some attempts their team made to address these challenges. A possible extension to this paper is discussions on whether code written in languages with tighter ecosystems such as Apple's Objective-C are easier for static analysis in the field.

The survey paper by Szekeres et al. \cite{szekeres2013sok} summarised common mitigations against memory corruption bugs. For each mitigation technique, they described associated performance overheads, compatibility issues, and attack vectors. Starting by introducing common types of memory corruption attacks, the authors gave an overview on protection systems in use based on both probabilistic protection and deterministic protection principles. After discussing important metrics and tradeoffs in cost and compatibility, they described in more detail how current systems perform in these aspects. They also covered defences against generic attacks and control-flow hijack attacks. The authors concluded by highlighting performance and compatibility constraints remain in current systems, which are preventing their wide adoption. A critique for this paper concerns on its lack of clarity in structure. For example, description and discussion of stack smashing protection were repeated twice \cite[III, VIII-B]{szekeres2013sok}.

\section{Key themes of research}

\subsection{Laboratory theory versus real-world practice}

When a system or technique evaluates well under laboratory conditions, it may not perform as well, or face operational difficulties in the field, inhibiting its wide adoption. An assumption made during seL4's verification process concerns the uniprocessor runtime environment \cite[3.3]{klein2009sel4}, which was still dominating the market at the time. With modern processors featuring multiple cores, seL4's applicability becomes severely constrained. Bessey et al. \cite[pp. 69-70]{bessey2010few} noted how compiler deviations from language standards caused difficulties in their static analysis model, requiring significant adaptation. Szekeres et al. reasoned that slow industrial adoption of memory corruption defences was caused by significant performance penalties and incompatibilities associated \cite[I, IV-C]{szekeres2013sok}.

\subsection{Security versus cost}

Strong resistance to wider use of software verification and error mitigation comes from the potential cost involved for stakeholders. In the case of the verified microkernel by Klein et al. \cite[5.3]{klein2009sel4}, fundamental redesigns to existing kernel features are very costly to reverify. Kernel redesigns are unfortunately a common occurrence in the industry \cite{israeli2010linux}. Bessey et al. \cite[p. 73]{bessey2010few} noted how upgraded systems leading to more bugs being identified can be bad for managers, as this can damage their bonuses. For performance-related cost, Szekeres et al. \cite[V-A]{szekeres2013sok} raised the example of a 10\% performance penalty preventing most programs from being compiled as Position Independent Executables (PIE), which would allow greater protection against memory corruption.

\subsection{Benefits of using an intermediate representation}

A more subtle theme seen across the three papers involves potential benefits of using intermediate representations in analysis. Klein et al. \cite[2.2, 5.2]{klein2009sel4} used a Haskell prototype as an executable model prior to optimisation in C, which saved significant cost in verification and allowed them to take advantages of existing toolchains and experience. When analysing C\# and Java bytecode compiled from the source, there was no need to navigate compiler deviations, therefore Bessey et al. \cite[p. 72]{bessey2010few} had a much easier time. Techniques providing the highest level of memory corruption protection (SoftBound+CETS) represented new objects with unique identifiers in processing to improve memory performance \cite[VI-C]{szekeres2013sok}. 

\section{Ideas in current context}

In order to address the limitations of seL4's uniprocessor model, a follow up research by Tessin \cite{von2012clustered} presented the \emph{clustered multikernel} approach for constructing verifiable multiprocessor kernels. In addition, Tessin developed a ``lifting framework'' to add multiprocessor support to verified uniprocessor microkernels with only minor modifications. This approach could save a large amount of effort required for developing a verifiable multiprocessor kernel from scratch.

Intermediate representations have been further utilised to improve program analysis when searching for errors and security violations, as demonstrated by the Static Analysis Intermediate Language (SAIL) \cite{dillig2009sail}. The two-level intermediary design of SAIL preserve syntactical error reporting capabilities, while allowing semantical analysis to be performed for error finding. A similar intermediate representation scheme was used by Dullien and Porst \cite{dullien2009reil} to perform platform-independent security auditing and vulnerability detection.

Bessey et al. \cite{bessey2010few} faced significantly problems when a varying range of C/C++ compilers failed to strictly observe language standards, creating false-positive error reports. An alternative error-finding approach has since emerged, combining the classical Bounded Model Checking (BMC) approach with LLVM intermediate representation to offset complexities of modern language features in C/C++, which previously made applying BMC difficult \cite{merz2012llbmc}. This approach is also able to find bugs introduced by compilers -- addressing another complaint by Bessey et al.

\section{Literature review}

While the correctness of seL4 microkernel was proved by Klein et al. \cite{klein2009sel4} on the design level, its high-level security properties of confidentiality and integrity were also proved by Murray et al. \cite{murray2013sel4} in 2013. The seL4's memory management system based on capabilities was modelled upon by Baumann et al. \cite{baumann2009multikernel} when designing their scalable multicore kernel, albeit without formal verification. In order to address the security problems caused by compiler errors when compiling verified C code such as seL4, Yang et al. \cite{yang2011finding} developed a method for detecting compiler bugs through randomised test-case generation. 

Considering the difficulties faced by Bessey et al. \cite{bessey2010few} in performing static analysis on unseen codebases, Chipounov et al.  \cite{chipounov2011s2e} proposed an alternative method of performing multi-path analysis directly on binaries. While their approach solved problems in accessing source code, it lacks error reporting capabilities due to the compiled nature of binaries. Johnson et al. \cite{johnson2013don} further investigated reasons behind developers not using static analysis to find bugs, including false positives and developer overloading, and made their recommendations to address these issues. Bodden et al. \cite{bodden2011taming} improved coverage of static analysis in Java by inserting runtime checks for unanalysed reflective calls.

Szekeres participated in a more recent work \cite{kuznetsov2014code}, which introduced new design concepts of code-pointer integrity and code-pointer separation, which supplemented defences against control-flow hijack attacks. Schuster et al. \cite{schuster2015vc3} developed an alternative protection principle of \emph{region self-integrity} for distributed MapReduce computations, in lieu of full memory safety protection, as full memory safety protection is very computationally-expensive as measured by Szekeres et al. \cite{szekeres2013sok}. 

\emph{(1189 words according to texcount.)}

\bibliographystyle{IEEEtran}
\footnotesize{\bibliography{week8}}


\end{document}
